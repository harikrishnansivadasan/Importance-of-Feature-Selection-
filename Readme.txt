# ğŸ§  Importance of Feature Selection in Machine Learning

A **comparative study and analysis** exploring how various **Feature Selection (FS)** techniques impact model accuracy, training efficiency, and interpretability across multiple datasets and classifiers.

Built with ğŸ Python, ğŸ“Š scikit-learn, and ğŸ” statistical analysis â€” this project demonstrates the **power of dimensionality reduction** in real-world machine learning workflows.

---

## ğŸš€ Features

- âš™ï¸ **Comparative Analysis** of multiple feature selection methods (Filter, Wrapper, Embedded)
- ğŸ¤– **Multi-Classifier Evaluation** â€” KNN, Decision Tree, and SVM
- ğŸ“ˆ **Dataset Variety** â€” Iris, Diabetes (Pima), and Wine datasets
- ğŸ§© **Feature Importance Visualization** using Random Forests and Decision Trees
- ğŸ“Š **Performance Benchmarking** with accuracy charts and confusion matrices
- ğŸ“‰ **Dimensionality Reduction Insights** for efficiency and interpretability

---

## ğŸ§± Tech Stack

| Category | Tools |
| :--- | :--- |
| **Language** | Python |
| **Machine Learning** | scikit-learn |
| **Data Handling** | pandas, numpy |
| **Visualization** | matplotlib |

---

## ğŸ”¬ Methodology Overview

This project performs a **3-way comparative analysis** using three datasets, three classifiers, and three categories of feature selection algorithms.

### ğŸ§  Feature Selection Algorithms

| Method Type | Description | Algorithms Used |
| :--- | :--- | :--- |
| **Filter Methods** | Selects features based on statistical measures (e.g., correlation) independent of model. | Random Forest & Decision Tree Importance |
| **Wrapper Methods** | Evaluates feature subsets based on model performance. | Recursive Feature Elimination (RFE) |
| **Embedded Methods** | Performs selection during model training (e.g., via regularization). | Lasso / Ridge Regression |

### âš™ï¸ Classifiers Tested
- K-Nearest Neighbors (KNN)  
- Decision Tree Classifier  
- Support Vector Machine (SVM)

### ğŸ§© Datasets Used

| Dataset | Type | Key Attributes |
| :--- | :--- | :--- |
| **Iris** | Multivariate | Petal Length, Petal Width, Sepal Length, Sepal Width |
| **Diabetes (Pima)** | Numeric | Pregnancies, Glucose, BMI, Age, etc. |
| **Wine** | Categorical / Complex | Fixed acidity, Alcohol, Total sulfur dioxide, etc. |

---

## âœ… Key Results

| Dataset | Best Feature Selection | Best Classifier |
| :--- | :--- | :--- |
| **Iris** | Recursive Feature Elimination (RFE) | KNN |
| **Diabetes** | Decision Tree Importance | SVM |
| **Wine** | Random Forest Importance | SVM |

> ğŸ† The optimal combination of Feature Selection and Classifier **depends on dataset characteristics** â€” no single method dominates all scenarios.

---

## ğŸ“Š Visual Results

### Iris Dataset Accuracy
![Accuracy Chart of Iris Data](Picture2.png)

### Diabetes Dataset Accuracy
![Accuracy Chart of Diabetes Data](Picture1.png)

### Wine Dataset Accuracy
![Accuracy Chart of Wine Data](Picture3.png)

---

## ğŸ› ï¸ Setup Instructions

1. **Clone the repo**
   ```bash
   git clone https://github.com/yourusername/FeatureSelectionStudy.git
   cd FeatureSelectionStudy
